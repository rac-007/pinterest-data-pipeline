{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Clean df_pin Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the necessary transformations on the follower_count to ensure every entry is a number. Make sure the data type of this column is an int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pin = df_pin.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"k\", \"000\"))\n",
    "df_pin = df_pin.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"M\", \"000000\"))\n",
    "# cast follower_count column to integer type\n",
    "df_pin = df_pin.withColumn(\"follower_count\", col(\"follower_count\").cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace irrelevant values with None across all columns\n",
    "irrelevant_values = [\"\", \"NA\", \"N/A\", \"null\"]\n",
    "df_pin = df_pin.replace(irrelevant_values, None)\n",
    "#display(df_pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the save_location by removing \"Local save in \"\n",
    "df_pin = df_pin.withColumn(\"save_location\", regexp_replace(col(\"save_location\"), r\"Local save in \", \"\"))\n",
    "#display(df_pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the 'index' column to 'ind'\n",
    "df_pin = df_pin.withColumnRenamed(\"index\", \"ind\")\n",
    "#display(df_pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns\n",
    "display(df_pin)\n",
    "new_df_pin_column_order = [\n",
    "    \"ind\",\n",
    "    \"unique_id\",\n",
    "    \"title\",\n",
    "    \"description\",\n",
    "    \"follower_count\",\n",
    "    \"poster_name\",\n",
    "    \"tag_list\",\n",
    "    \"is_image_or_video\",\n",
    "    \"image_src\",\n",
    "    \"save_location\",\n",
    "    \"category\"\n",
    "]\n",
    "df_pin = df_pin.select(new_df_pin_column_order)\n",
    "#df_pin.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# display changes\n",
    "df_pin.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean df_geo dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = \"/mnt/aws-bucket/topics/129bc7e0bd61.geo/partition=0/\"\n",
    "file_type = \"json\"\n",
    "infer_schema = \"true\"\n",
    "# Read in JSONs from mounted S3 bucket\n",
    "df_geo = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "display(df_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, col\n",
    "# Create a new column 'coordinates' that contains an array with latitude and longitude\n",
    "df_geo = df_geo.withColumn(\"coordinates\", array(col(\"latitude\"), col(\"longitude\")))\n",
    "display(df_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the latitude and longitude columns from the DataFrame\n",
    "df_geo = df_geo.drop(\"latitude\", \"longitude\")\n",
    "display(df_geo)\n",
    "df_geo.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the timestamp column from a string to a timestamp data type\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "df_geo = df_geo.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reorder the DataFrame columns\n",
    "df_geo = df_geo.select(\"ind\",\"country\",\"coordinates\",\"timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean df_user Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = \"/mnt/aws-bucket/topics/129bc7e0bd61.user/partition=0/\"\n",
    "file_type = \"json\"\n",
    "infer_schema = \"true\"\n",
    "# Read in JSONs from mounted S3 bucket\n",
    "df_user = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "display(df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, col, lit\n",
    "#Create a new column user_name that concatenates the information found in the first_name and last_name columns\n",
    "df_user = df_user.withColumn(\"user_name\", concat(col(\"first_name\"), lit(\" \"), col(\"last_name\")))\n",
    "df_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the first_name and last_name columns from the DataFrame\n",
    "df_user = df_user.drop(\"first_name\", \"last_name\")\n",
    "df_user.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the date_joined column from a string to a timestamp data type\n",
    "df_user = df_user.withColumn(\"date_joined\", col(\"date_joined\").cast(\"timestamp\"))\n",
    "df_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reorder the DataFrame columns\n",
    "df_user = df_user.select(\"ind\", \"user_name\",\"age\",\"date_joined\")\n",
    "df_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# display changes\n",
    "df_user.printSchema()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
